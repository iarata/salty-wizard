{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f3696c",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Add parent directory to path to import brain_text_model\n",
    "sys.path.insert(0, os.path.join(os.path.dirname('__file__'), '..'))\n",
    "from brain_text_model.tokenizer import CharTokenizer, TokenizerConfig\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = CharTokenizer(TokenizerConfig())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a05f25",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054fbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_image_paths(labels: dict):\n",
    "    \"\"\"\n",
    "    Function that turns a dictionary mapping an image path to a cluster label,\n",
    "    into mapping cluster label to list of paths\n",
    "    \"\"\"\n",
    "    groups = {}\n",
    "\n",
    "    for key, value in labels.items():\n",
    "        if value not in groups:\n",
    "            groups[value] = []\n",
    "        groups[value].append(key)\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f29c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(cluster_paths: list, super_path: str = ''):\n",
    "    \"\"\"\n",
    "    Function that load the images form the paths provided in the list\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for path in cluster_paths:\n",
    "        # Load image\n",
    "        img = plt.imread(super_path + '_'.join(path.split('_')[:2]))\n",
    "        data.append(img)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b320b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_data_from_image_path(cluster_paths: list):\n",
    "    \"\"\"\n",
    "    Extracts the sentences used to generate the images from the provided list of paths\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    \n",
    "    for path in cluster_paths:\n",
    "        s = path.split(\"\\\\\")[1].split(\"_\")[0]\n",
    "        s = ' '.join(s.split('-'))\n",
    "        sentences.append(s)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_data(cluster_paths: list, data_path: str = ''):\n",
    "    \"\"\"\n",
    "    Extracts the sentences from HDF5 files based on the provided paths.\n",
    "    Each path is split/date/trial, where trial is the row ID in the HDF5 file.\n",
    "    \"\"\"\n",
    "    import h5py\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Group paths by file to minimize file I/O\n",
    "    file_to_trials = defaultdict(list)\n",
    "    path_order = []  # Track original order\n",
    "    \n",
    "    for path in cluster_paths:\n",
    "        split, date, trial = path.split(\"/\")\n",
    "        file_path = os.path.join(data_path, date, f'data_{split}.hdf5')\n",
    "        \n",
    "        if trial not in file_to_trials[file_path]:\n",
    "            file_to_trials[file_path].append((trial, len(path_order)))\n",
    "            path_order.append(None)  # Placeholder\n",
    "    \n",
    "    # Read files once and extract required rows\n",
    "    for file_path, trial_list in file_to_trials.items():\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "\n",
    "            for trial_id, original_idx in trial_list:\n",
    "                # Access the row with the given trial_id                \n",
    "                trial_id = '_'.join(trial_id.split('_')[:2])  # Adjust trial_id format\n",
    "                \n",
    "                group = f[trial_id]\n",
    "                attrs = {key: group.attrs[key] for key in group.attrs}\n",
    "                try:\n",
    "                    sentence = attrs.get(\"sentence_label\")\n",
    "                except KeyError:\n",
    "                    sentence = []\n",
    "                # Decode if bytes\n",
    "                if isinstance(sentence, bytes):\n",
    "                    sentence = sentence.decode('utf-8')\n",
    "                # Remove punctuation if any\n",
    "                sentence = sentence.replace('.', '').replace(',', '').replace('!', '').replace('?', '')\n",
    "                # Remove stop words\n",
    "                sentence = ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n",
    "                path_order[original_idx] = tokenizer.decode(tokenizer.encode(sentence)).split()\n",
    "                # print(f\"Extracted sentence: {path_order[original_idx]}\")\n",
    "    return path_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(sentences: list):\n",
    "    \"\"\"\n",
    "    Gets list of sentences and uses map reduce to count the number of occurences of each word\n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    # print(\"Calculating word frequency for sentences:\", sentences[0:3], \"...\")\n",
    "\n",
    "    flattened_words = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence is None:\n",
    "            continue\n",
    "        flattened_words.extend(sentence)\n",
    "\n",
    "    mapped_words = map(lambda w: (w, 1), flattened_words)\n",
    "\n",
    "    def reducer(acc, pair):\n",
    "        word, count = pair\n",
    "        acc[word] += count\n",
    "        return acc\n",
    "    \n",
    "    # Use reduce to aggregate word counts\n",
    "    word_freq = reduce(reducer, mapped_words, defaultdict(int))\n",
    "\n",
    "    # Convert defaultdict to a regular dictionary\n",
    "    return dict(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03881c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(cluster_data: dict):\n",
    "    \"\"\"\n",
    "    Function that calculates the tf-idf index of words in clusters\n",
    "\n",
    "    Input:\n",
    "    - cluster_data: dictionary of cluster number and a list of sentences in that cluster\n",
    "    \"\"\"\n",
    "    # Get word frequency of all clusters\n",
    "    freqs = {}\n",
    "    for cluster, sentences in cluster_data.items():\n",
    "        freqs[cluster] = word_frequency(sentences)\n",
    "\n",
    "    # Calculate Term Frequency in each cluster\n",
    "    #   count of word / max count of word\n",
    "    tf = {}\n",
    "    idf = {}\n",
    "\n",
    "    for cluster, w_freqs in freqs.items():\n",
    "\n",
    "        max_f = w_freqs[max(w_freqs, key=w_freqs.get)]\n",
    "\n",
    "        cluster_tfs  = {}\n",
    "        cluster_idfs = {}\n",
    "\n",
    "        for word, count in w_freqs.items():\n",
    "            cluster_tfs[word] = count/max_f\n",
    "            \n",
    "            count = 0\n",
    "            for _, words_dict in freqs.items():\n",
    "                if word in words_dict:\n",
    "                    count += 1\n",
    "\n",
    "            cluster_idfs[word] = math.log(len(freqs.keys())/count, 2)\n",
    "        \n",
    "        tf[cluster]  = cluster_tfs\n",
    "        idf[cluster] = cluster_idfs\n",
    "\n",
    "    # Calculate TF-IDF\n",
    "    tf_idf = {}\n",
    "    for cluster in tf:\n",
    "        cluster_tf_idf = {}\n",
    "        for word in tf[cluster]:\n",
    "            cluster_tf_idf[word] = tf[cluster][word] * idf[cluster][word]\n",
    "        tf_idf[cluster] = cluster_tf_idf\n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd176539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_trial_frequency(cluster_data: dict):\n",
    "    \"\"\"\n",
    "    Function that calculates the frequency of trials in each cluster\n",
    "\n",
    "    Input:\n",
    "    - cluster_data: dictionary of cluster number and a list of sentences in that cluster\n",
    "    \"\"\"\n",
    "    trial_freq_clusters = {}\n",
    "\n",
    "    for cluster in cluster_data:\n",
    "        cluster_paths = cluster_data[cluster]\n",
    "        \n",
    "        trial_freq = {}\n",
    "\n",
    "        for path in cluster_paths:\n",
    "            _, date, trial = path.split(\"/\")\n",
    "            trial_id = '_'.join(trial.split('_')[:2])\n",
    "            key = '/'.join([_, date, trial_id])\n",
    "            if key not in trial_freq:\n",
    "                trial_freq[key] = 0\n",
    "            trial_freq[key] += 1\n",
    "\n",
    "        trial_freq_clusters[cluster] = trial_freq\n",
    "\n",
    "    return trial_freq_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def davies_bouldin_index(data: list, labels: list):\n",
    "    \"\"\"\n",
    "    Function that calculates the davies bouldin index of the provided data and labels\n",
    "    \"\"\"\n",
    "    return davies_bouldin_score(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjusted_rand_index_calc(cluster_data1: dict, cluster_data2: dict):\n",
    "    \"\"\"\n",
    "    Function that calculates the Adjusted Rand Index (ARI) between two clusterings.\n",
    "\n",
    "    Inputs:\n",
    "    - cluster_data1: dict mapping cluster_id -> list of item identifiers (e.g. image paths)\n",
    "    - cluster_data2: dict with the same structure and the same set of item identifiers\n",
    "\n",
    "    Returns:\n",
    "    - float: Adjusted Rand Index\n",
    "    \"\"\"\n",
    "\n",
    "    def invert(clusters: dict):\n",
    "        mapping = {}\n",
    "        for label, items in clusters.items():\n",
    "            for it in items:\n",
    "                mapping[it] = label\n",
    "        return mapping\n",
    "\n",
    "    m1 = invert(cluster_data1)\n",
    "    m2 = invert(cluster_data2)\n",
    "\n",
    "    if set(m1.keys()) != set(m2.keys()):\n",
    "        raise ValueError(\"Both clusterings must contain the same set of items.\")\n",
    "\n",
    "    items = sorted(m1.keys())  # deterministic ordering\n",
    "    labels1 = [m1[it] for it in items]\n",
    "    labels2 = [m2[it] for it in items]\n",
    "\n",
    "    return adjusted_rand_score(labels1, labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_trial_frequency(trial_freq: dict):\n",
    "    \"\"\"\n",
    "    Function that creates a box plot of the trial frequencies in each cluster\n",
    "\n",
    "    Input:\n",
    "    - cluster_data: dictionary of cluster number and a list of sentences in that cluster\n",
    "    \"\"\"\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for cluster, freq_dict in trial_freq.items():\n",
    "        freqs = list(freq_dict.values())\n",
    "        data.append(freqs)\n",
    "        labels.append(str(cluster))\n",
    "\n",
    "    plt.boxplot(data, labels=labels)\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Trial Count')\n",
    "    plt.title('Number of same trial bins in clusters')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c498a",
   "metadata": {},
   "source": [
    "## Main Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cebc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cluster_file: str = 'src\\\\clustering\\\\across_trial_clusters_raw.pkl'):\n",
    "\n",
    "    # Load per_trial_clusters_raw.pkl\n",
    "    with open(cluster_file, 'rb') as f:\n",
    "        import pickle\n",
    "        per_trial_clusters = pickle.load(f)\n",
    "    \n",
    "    clusters = per_trial_clusters\n",
    "\n",
    "    print(\"Number of clusters:\", len(clusters))\n",
    "    print(clusters.keys())\n",
    "    \n",
    "    path = 'src/data/hdf5_data_final'\n",
    "\n",
    "    trial_freq = cluster_trial_frequency(clusters)\n",
    "\n",
    "    box_plot_trial_frequency(trial_freq)\n",
    "    \n",
    "    new_clusters = defaultdict(list)\n",
    "    # Keep trial in cluster if most frequent in reverse order to remove items\n",
    "    for cluster, freq_dict in trial_freq.items():\n",
    "        for trial, freq in freq_dict.items():\n",
    "        \n",
    "            other_max = 0\n",
    "            for cluster2 in trial_freq.keys():\n",
    "                if cluster2 == cluster:\n",
    "                    continue\n",
    "                try:\n",
    "                    if trial_freq[cluster2][trial] > other_max:\n",
    "                        other_max = trial_freq[cluster2][trial]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if freq > other_max:\n",
    "                new_clusters[cluster].append(trial)\n",
    "\n",
    "\n",
    "    data_img = {}\n",
    "    data_txt = {}\n",
    "    for cluster, paths in new_clusters.items():\n",
    "        # data_img[cluster] = get_image_data(paths, super_path=path)\n",
    "        data_txt[cluster] = get_sentence_data(paths, data_path=path)\n",
    "\n",
    "    # Sanity check\n",
    "    # print(data_txt)\n",
    "\n",
    "    tf_idf_indices = tf_idf(data_txt)\n",
    "    \n",
    "    # SOrt words by tf-idf value and print top 5\n",
    "    for cluster, indices in tf_idf_indices.items():\n",
    "        sorted_words = sorted(indices.items(), key=lambda item: item[1], reverse=True)\n",
    "        print(f\"Cluster {cluster} top words:\")\n",
    "        for word, score in sorted_words[:5]:\n",
    "            print(f\"  {word}: {score:.4f}\")\n",
    "    \n",
    "    # print(\"Davies-Bouldin Index:\")\n",
    "    # db_index = davies_bouldin_index(\n",
    "    #     data=[img.flatten() for cluster in data_img.values() for img in cluster],\n",
    "    #     labels=[int(cluster) for cluster, paths in clusters.items() for _ in paths]\n",
    "    # )\n",
    "    # print(db_index)\n",
    "    return new_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6dfbbd",
   "metadata": {},
   "source": [
    "## Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cluster = main()\n",
    "\n",
    "for cluster, paths in new_cluster.items():\n",
    "    print(f\"Cluster {cluster} has {len(paths)} items.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
